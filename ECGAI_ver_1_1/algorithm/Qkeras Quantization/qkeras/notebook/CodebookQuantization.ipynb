{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Copyright 2020 Google LLC\n",
    "#\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codebook based quantization\n",
    "\n",
    "Codebook based quantizaion is a non-uniform quantization technique that maps each weight or activation value to the index of a value in the codebook. This allows us to compress weights/activations even further with neglibible loss in performance. We will demonstrate this by training an object classification model and applying codebook quantization to the activation with the most values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.datasets import *\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from qkeras import *\n",
    "from qkeras.codebook import *\n",
    "\n",
    "\n",
    "def get_data(name, sample_size=1.0):\n",
    "  (x_train, y_train), (x_test, y_test) = globals()[name].load_data()\n",
    "\n",
    "  if len(x_train.shape) == 3:\n",
    "    x_train = x_train.reshape(x_train.shape + (1,))\n",
    "    x_test = x_test.reshape(x_test.shape + (1,))\n",
    "\n",
    "  x_train = x_train.astype(\"float32\")\n",
    "  x_test = x_test.astype(\"float32\")\n",
    "\n",
    "  mean = np.mean(x_train,axis=(0,1,2,3))\n",
    "  std = np.std(x_train,axis=(0,1,2,3))\n",
    "  x_train = (x_train-mean)/(std+1e-7)\n",
    "  x_test = (x_test-mean)/(std+1e-7)\n",
    "\n",
    "  y_train_c = to_categorical(y_train, np.max(y_train) + 1)\n",
    "  y_test_c = to_categorical(y_test, np.max(y_test) + 1)\n",
    "\n",
    "  if sample_size != 1.0:\n",
    "    indexes = np.asarray(range(x_train.shape[0]))\n",
    "    np.random.shuffle(indexes)\n",
    "    indexes = indexes[:int(x_train.shape[0] * sample_size)]\n",
    "\n",
    "    x_train = x_train[indexes]\n",
    "    y_train_c = y_train_c[indexes]\n",
    "\n",
    "  return (x_train, y_train_c), (x_test, y_test_c)\n",
    "\n",
    "\n",
    "def get_model(\n",
    "  name, X_train, y_train, X_test, y_test,\n",
    "  blocks=[[32], [64], [128]],\n",
    "  quantizer_list=[\n",
    "      \"quantized_relu_po2(4,4)\",\n",
    "      \"quantized_relu_po2(4,4)\"\n",
    "  ],\n",
    "  use_stochastic_rounding=0,\n",
    "  l1v=None,\n",
    "  epochs=10,\n",
    "  load_weights=True):\n",
    "\n",
    "  if l1v is None:\n",
    "    l1v = [0.0] * len(blocks)\n",
    "\n",
    "  X_shape = X_train.shape[1:]\n",
    "  x_i = x = Input(X_shape)\n",
    "\n",
    "  for b, block in enumerate(blocks):\n",
    "    # we are assuming we want to quantize the block that has sparsity\n",
    "    # so let's add dropout to the next layer\n",
    "\n",
    "    if b >= 1 and l1v[b-1] != 0.0:\n",
    "      x = Dropout(0.3, name=f\"drop{b}\")(x)\n",
    "\n",
    "    for i in range(len(block)):\n",
    "      x = QConv2D(\n",
    "          block[i], kernel_size=(3,3), strides=(2,2), padding=\"same\",\n",
    "          kernel_quantizer=f\"quantized_bits(4, use_stochastic_rounding={use_stochastic_rounding})\",\n",
    "          bias_quantizer=f\"quantized_po2(4, use_stochastic_rounding={use_stochastic_rounding})\",\n",
    "          kernel_regularizer=l1(l1v[b]) if l1v[b] != 0.0 else None,\n",
    "          name=f\"d{b}_{i}\")(x)\n",
    "      if i != len(block) - 1:\n",
    "        if quantizer_list[b] in [\"linear\", \"relu\", \"softmax\", \"sigmoid\"]:\n",
    "          x = Activation(quantizer_list[b], name=f\"a{b}_{i}\")(x)\n",
    "        else:\n",
    "          x = QActivation(quantizer_list[b], name=f\"a{b}_{i}\")(x)\n",
    "      else:\n",
    "        x = QBatchNormalization(name=f\"bn{b}_{i}\")(x)\n",
    "    if b < len(blocks) - 1:\n",
    "      if quantizer_list[b] in [\"linear\", \"relu\", \"softmax\", \"sigmoid\"]:\n",
    "        x = Activation(quantizer_list[b], name=f\"a{b}_{len(block)-1}\")(x)\n",
    "      else:\n",
    "        x = QActivation(quantizer_list[b], name=f\"a{b}_{len(block)-1}\")(x)\n",
    "    else:\n",
    "      if len(block) > 0:\n",
    "        x = QActivation(f\"quantized_relu(6,2, use_stochastic_rounding={use_stochastic_rounding})\", \n",
    "                        name=f\"a{b}_{len(block)-1}\")(x)\n",
    "      x = Flatten(name=\"flatten\")(x)\n",
    "      x = QDense(\n",
    "          y_train.shape[1], name=f\"d{len(blocks)-1}_{len(block)}\")(x)\n",
    "      x = Activation(\"softmax\", name=f\"a{len(blocks)-1}_{len(block)}\")(x)\n",
    "\n",
    "  model = Model(inputs=x_i, outputs=x)\n",
    "  model.summary()\n",
    "\n",
    "  model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(0.001), metrics=[\"acc\"])\n",
    "\n",
    "  try:\n",
    "    if load_weights and os.path.isfile(name + \".h5\"):\n",
    "      print('Found file...')\n",
    "      model.load_weights(name + \".h5\")\n",
    "    else:\n",
    "      model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "                batch_size=128, epochs=epochs, verbose=2)\n",
    "      model.save_weights(name + \".h5\")\n",
    "  except:\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "              batch_size=128, epochs=epochs, verbose=2)\n",
    "    model.save_weights(name + \".h5\")\n",
    "\n",
    "  return model\n",
    "\n",
    "\n",
    "name = \"cifar10\"\n",
    "(X_train, y_train), (X_test, y_test) = get_data(name, sample_size=1)\n",
    "model = get_model(\n",
    "  name, X_train, y_train, X_test, y_test,\n",
    "  blocks=[[32, 32], [64, 64], [128]],\n",
    "  quantizer_list=[\"quantized_relu(6,2)\", \"quantized_relu(6,2)\"],\n",
    "  epochs=50,\n",
    "  load_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qkeras.codebook import *\n",
    "\n",
    "cb_tables, models, km_models = activation_compression(\n",
    "  model, \n",
    "  {'loss' : \"categorical_crossentropy\", 'metrics' : [\"acc\"]},\n",
    "  [2], 3, \n",
    "  X_train, y_train, \n",
    "  X_test, y_test,\n",
    "  sample_size=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = models[0].layers[-1].quantizer\n",
    "in_table, out_table = cb_tables[0]\n",
    "print(q)\n",
    "print('in_table:', in_table)\n",
    "print('out_table:', out_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i,x in enumerate(q.range()):\n",
    "  print(f'{x:8}, {in_table[out_table[i]]:6}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight compression using codebook quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_weights = model.layers[1].weights[0].numpy()\n",
    "print(conv_weights.shape)\n",
    "quantizer = model.layers[1].kernel_quantizer_internal\n",
    "print(quantizer)\n",
    "axis = 3\n",
    "bits = 3\n",
    "index_table, codebook_table = weight_compression(\n",
    "  conv_weights, \n",
    "  bits, \n",
    "  axis, \n",
    "  quantizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(codebook_table.shape)\n",
    "codebook_table[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(index_table.shape)\n",
    "index_table[:,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_conv_weights = np.zeros(conv_weights.shape)\n",
    "for i in range(conv_weights.shape[axis]):\n",
    "  new_conv_weights[:,:,:,i] = codebook_table[i][index_table[:,:,:,i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_conv_weights[:,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_weights[:,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = model.layers[1].weights[1].numpy()\n",
    "model.layers[1].set_weights([new_conv_weights, bias])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
